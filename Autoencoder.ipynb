{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Autoencoder Model\n",
    "\n",
    "*This is my submission for Google Code-in's Tensorflow Autoencoder challenge.*\n",
    "\n",
    "This notebook walks you through how to create a simple autoencoder using Keras that can encode and decode images in the MNIST dataset. Enjoy :)\n",
    "\n",
    "Author: Adar Kahiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "#Importing dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Reshape, Conv2DTranspose\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing & Normalizing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "train_images, _ = train #It's not necessary to keep the labels for autoencoders\n",
    "test_images, _ = test\n",
    "\n",
    "#Normalizing images to between 0 and 1. Dividing by 255 (the max brightness of a pixel) ensures this. \n",
    "train_images, test_images = np.divide(train_images, 255), np.divide(test_images, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing A Batch Of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb37690ef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(train_images[0], cmap='gray') #Printing the first training image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (60000, 28, 28) \n",
      "Output shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Converting the data from numpy arrays to TensorFlow tensors\n",
    "train_images = tf.convert_to_tensor(train_images)\n",
    "test_images = tf.convert_to_tensor(test_images)\n",
    "\n",
    "print(\"Training shape: {} \\nOutput shape: {}\".format(train_images.shape, test_images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 16)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 24)          1560      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1176)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                37664     \n",
      "=================================================================\n",
      "Total params: 39,304\n",
      "Trainable params: 39,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1176)              38808     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 24)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 16)        1552      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 1)         65        \n",
      "=================================================================\n",
      "Total params: 40,425\n",
      "Trainable params: 40,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ------------- ENCODER ------------- #\n",
    "encoder = Sequential() #Initializing the encoder\n",
    "encoder.add(Conv2D(16, kernel_size=2, input_shape=(28, 28, 1), padding='same', strides=2, activation='relu' )) #16 filters, 2x2 kernels, stride of 2, and ReLu activation function. \n",
    "encoder.add(Conv2D(24, kernel_size=2, padding='same', strides=2, activation='relu')) #24 filters, 2x2 kernels, stride of 2, and ReLu activation function. \n",
    "encoder.add(Flatten()) #Flattening the image so it can be put through a dense layer\n",
    "encoder.add(Dense(32, input_shape=(7*7*24,), activation='relu')) #Input shape is obtained from the dimensions of each image multiplied by # of convolutional layers beforehand.\n",
    "\n",
    "\n",
    "# ------------- ENCODER ------------- #\n",
    "'''\n",
    "This is basically the reverse of the encoder. It will take the input generated by the final linear (dense)\n",
    "of the encoder, and convert it back to its original form. If it can do this well, we'll know that the embedding\n",
    "generated by the encoder was accurate. \n",
    "'''\n",
    "decoder = Sequential() #Initializing the decoder\n",
    "decoder.add(Dense(7*7*24, input_shape=(32,), activation='relu'))\n",
    "decoder.add(Reshape((7, 7, 24)))\n",
    "decoder.add(Conv2DTranspose(16, kernel_size=2, padding='same', strides=2, activation='relu'))\n",
    "decoder.add(Conv2DTranspose(1, kernel_size=2, padding='same', strides=2, activation='relu'))\n",
    "\n",
    "#Output a 'model summary'. This is useful for debugging since it shows us the shape of the output at each step. \n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the composite model (the complete autoencoder)\n",
    "Right now we have two separate models. In order to train them as one model, we can use the Functional API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 32)                39304     \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 28, 28, 1)         40425     \n",
      "=================================================================\n",
      "Total params: 79,729\n",
      "Trainable params: 79,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Creating the model graph\n",
    "img_input = Input(shape=(28, 28, 1))\n",
    "encoder_output = encoder(img_input) #Feeding image input into encoder model\n",
    "decoder_output = decoder(encoder_output) #Feeding the encoder output into the decoder model\n",
    "\n",
    "#Defining the model\n",
    "autoencoder = Model(inputs=img_input, outputs=decoder_output) #\n",
    "autoencoder.summary() #Display the shape of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#Adding a channel dimension of 1 to input\n",
    "train_images = tf.expand_dims(train_images, -1)\n",
    "test_images = tf.expand_dims(test_images, -1)\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/16\n",
      "60000/60000 [==============================] - 15s 246us/sample - loss: 0.1848 - binary_crossentropy: 0.1848\n",
      "Epoch 2/16\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.1499 - binary_crossentropy: 0.1499\n",
      "Epoch 3/16\n",
      "60000/60000 [==============================] - 13s 224us/sample - loss: 0.1372 - binary_crossentropy: 0.1372\n",
      "Epoch 4/16\n",
      "60000/60000 [==============================] - 14s 227us/sample - loss: 0.1343 - binary_crossentropy: 0.1343\n",
      "Epoch 5/16\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 0.1360 - binary_crossentropy: 0.1360\n",
      "Epoch 6/16\n",
      "60000/60000 [==============================] - 14s 230us/sample - loss: 0.1288 - binary_crossentropy: 0.1288\n",
      "Epoch 7/16\n",
      "60000/60000 [==============================] - 15s 246us/sample - loss: 0.1265 - binary_crossentropy: 0.1265\n",
      "Epoch 8/16\n",
      "60000/60000 [==============================] - 14s 237us/sample - loss: 0.1316 - binary_crossentropy: 0.1316\n",
      "Epoch 9/16\n",
      "60000/60000 [==============================] - 14s 230us/sample - loss: 0.1277 - binary_crossentropy: 0.1277\n",
      "Epoch 10/16\n",
      "60000/60000 [==============================] - 15s 245us/sample - loss: 0.1285 - binary_crossentropy: 0.1285\n",
      "Epoch 11/16\n",
      "60000/60000 [==============================] - 14s 230us/sample - loss: 0.1242 - binary_crossentropy: 0.1242\n",
      "Epoch 12/16\n",
      "60000/60000 [==============================] - 23s 388us/sample - loss: 0.1314 - binary_crossentropy: 0.1314\n",
      "Epoch 13/16\n",
      "60000/60000 [==============================] - 19s 309us/sample - loss: 0.1289 - binary_crossentropy: 0.1289\n",
      "Epoch 14/16\n",
      "60000/60000 [==============================] - 17s 280us/sample - loss: 0.1175 - binary_crossentropy: 0.1175\n",
      "Epoch 15/16\n",
      "60000/60000 [==============================] - 13s 221us/sample - loss: 0.1269 - binary_crossentropy: 0.1269\n",
      "Epoch 16/16\n",
      "60000/60000 [==============================] - 15s 244us/sample - loss: 0.1249 - binary_crossentropy: 0.1249\n"
     ]
    }
   ],
   "source": [
    "#Specifying hyperparamaters\n",
    "n_epochs = 30 #Number of epochs over which the model will train \n",
    "\n",
    "#Specifying loss function, metrics, and optimizer\n",
    "loss = tf.keras.losses.BinaryCrossentropy() #Using mean squared error loss\n",
    "metrics = tf.metrics.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam() #Using the Adam Optimizer\n",
    "\n",
    "\n",
    "#Compiling the model\n",
    "autoencoder.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "#Running the model\n",
    "history = autoencoder.fit(train_images, train_images, epochs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the index of the image in the test set you'd like to test: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = int(input(\"Enter the index of the image in the test set you'd like to test: \"))\n",
    "test_image = tf.expand_dims(test_images[idx], 0)\n",
    "new_image = autoencoder.predict(test_image)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "original = fig.add_subplot(1, 2, 1)\n",
    "original.imshow(test_images[idx].numpy().squeeze(), cmap='gray')\n",
    "original.set_title('Original')\n",
    "\n",
    "output = fig.add_subplot(1, 2, 2)\n",
    "output.imshow(new_image.squeeze(), cmap='gray') \n",
    "output.set_title('Output')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
